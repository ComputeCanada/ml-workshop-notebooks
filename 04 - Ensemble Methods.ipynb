{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods\n",
    "### Combiding models together to improve performance\n",
    "### Can act as regularization\n",
    "### With decision trees, provides the closest thing to \"It just works\" algorithm for a diversity of cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main ways of aggregating models:\n",
    "- **bagging**: train multiple copies of the same model on randomly subsamples datasets, average their output\n",
    "- **boosting**: train successive versions of a model on samples the aggregate of the previous fails on\n",
    "- **stacking**: train different models, and stack a meta-predictor using their outputs as inputs on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best known is probably random forests.\n",
    "# Technically, this is not a boosting algorithm, but is conceptually very similar\n",
    "# The idea: train a large bunch of very shallow, simple trees (that do not overfit), with random parameters.\n",
    "# i.e. looking at a different subset of features, taking different splits, etc.\n",
    "\n",
    "# let's see it in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import helper\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "X, Y, TRUE_SURFACE = helper.gen_planar_samples(seed=1344)\n",
    "plt.figure()\n",
    "ax_l = plt.subplot(1, 2, 1)\n",
    "helper.plot_red_blue(X, Y, ax=ax_l)\n",
    "ax_r = plt.subplot(1, 2, 2)\n",
    "helper.plot_decision_surface(TRUE_SURFACE, ax=ax_r)\n",
    "plt.gcf().set_size_inches(14, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a21f57043c4ca981c5d11ab4c51915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='max_depth', max=5, min=1), IntSlider(value=10, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def train_random_forest(max_depth=1, num_trees=10):\n",
    "    model = RandomForestClassifier(max_depth=max_depth, n_estimators=num_trees).fit(X, Y)\n",
    "    helper.plot_decision_surface(model.predict_proba)\n",
    "    helper.plot_red_blue(X, Y, ax=plt.gca())\n",
    "    plt.gcf().set_size_inches(7, 6)\n",
    "    \n",
    "interact(\n",
    "    train_random_forest,\n",
    "    max_depth=widgets.IntSlider(min=1, max=5, value=2),\n",
    "    num_trees=widgets.IntSlider(min=1, max=50, value=10),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3243d9df254827a4272006163bee31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='num_classifiers', min=1), Output()), _dom_classes=('wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# My backup career goal is to try and sell a print of the above to the NAC to replace the \"Voice of Fire\"\n",
    "# Now let's look at boosted trees:\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def train_adaboost(num_classifiers=10):\n",
    "    \n",
    "    model = AdaBoostClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=num_classifiers).fit(X, Y)\n",
    "    \n",
    "    helper.plot_decision_surface(model.predict_proba)\n",
    "    helper.plot_red_blue(X, Y, ax=plt.gca())\n",
    "    plt.gcf().set_size_inches(7, 6)\n",
    "    \n",
    "interact(\n",
    "    train_adaboost,\n",
    "    num_classifiers=widgets.IntSlider(min=1, max=100, value=10),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on AdaBoost\n",
    "\n",
    "Notice how the AdaBoostClassifier actually takes another predictor as the first argument.\n",
    "\n",
    "AdaBoosting is a general process that can be applied to arbitrary base classifiers. It's most often used with trees, however, since it needs to train very many version of a model, and only shallow trees are fast enough to train. \n",
    "\n",
    "You don't really want to do it with a neural net!\n",
    "\n",
    "### Weak Learners\n",
    "An important thing to note is that AdaBoost works best with \"weak learners\", defined as a simple model which is capable of achieving just above 50% accuracy (or just above 1/#Classes more generally). Without going into the details: if we can't do better than 50/50 out of any single learner, then aggregating them won't help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e83e068884045a991279f4035a82a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(RadioButtons(description='kernel', options=('linear', 'rbf'), value='linear'), IntSlider…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For kicks, let's see what adaboosting a linear SVM looks like...\n",
    "from sklearn.svm import SVC\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore', '.*')\n",
    "def train_adasvm(kernel='linear', num_classifiers=10):\n",
    "    \n",
    "    # adaboost needs probabilities to work\n",
    "    model = AdaBoostClassifier(\n",
    "        base_estimator=SVC(kernel=kernel, probability=True, gamma=2), n_estimators=num_classifiers).fit(X, Y)\n",
    "    \n",
    "    helper.plot_decision_surface(model.predict_proba)\n",
    "    helper.plot_red_blue(X, Y, ax=plt.gca())\n",
    "    plt.gcf().set_size_inches(7, 6)\n",
    "    \n",
    "interact(\n",
    "    train_adasvm,\n",
    "    num_classifiers=widgets.IntSlider(min=1, max=20, value=3),\n",
    "    kernel=widgets.RadioButtons(options=['linear', 'rbf'])\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the linear SVM\n",
    "This washed out mess is an illustration of why domain knowledge is important: the random samples are not linearly separable, and red and blue are generated in equal amounts. The end result is that trying to layer linear models results in every point being labelled 50/50. The linear classifier is simply not up to the task!\n",
    "### For the RBF SVM\n",
    "Think a little bit about how it works using the \"shining light\" analogy from the Decision Surfaces notebook. This is a potentially-useful classifier! (In any case, someone wrote a thesis about it: https://arxiv.org/pdf/0812.2575.pdf) though it does appear to suffer from \"washout\" as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
