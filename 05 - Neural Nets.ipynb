{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets\n",
    "- The new hotness.\n",
    "- Not so mysterious!\n",
    "- - not really like the human brain\n",
    "- - not magical\n",
    "- - they work by implicitly making a general and powerful assumption about their input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural nets use a different library ecosystem\n",
    "import keras\n",
    "import keras.layers as kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST is the \"generic toy dataset\" of choice for deep learning experiments\n",
    "from keras.datasets import mnist\n",
    "(X_tr, y_tr), (X_v, y_v) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what this data looks like:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as npr\n",
    "import numpy as np\n",
    "plt.imshow(X_tr[npr.randint(X_tr.shape[0])], cmap=\"Greys\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do one-hot encoding. Take labels of the form 1, 3, 6 into index-encoded\n",
    "# arrays like [0, 1, 0...]. [0, 0, 0, 1, 0...], etc.\n",
    "from keras.utils import to_categorical\n",
    "Y_tr = to_categorical(y_tr)\n",
    "Y_v = to_categorical(y_v)\n",
    "\n",
    "# otherwise normalization would do integer rounding: we don't want that!\n",
    "X_tr = X_tr.astype(np.float64)\n",
    "X_v = X_v.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural nets work best with normalized data.\n",
    "\n",
    "Normalization subtracts the mean and divides by the standard deviation.\n",
    "It's often applied \"featurewise\", i.e. each pixel is normalized relative to the\n",
    "pixels in the same locaiton across images\n",
    "\n",
    "In MNIST, because some pixels are almost always 0, normalizing them would lead to value blowup. We normalize by the global average instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: how should normalization happen with the separate training and validation sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "M = X_tr.mean()\n",
    "S = X_tr.std()\n",
    "    \n",
    "X_tr -= M\n",
    "X_tr /= S\n",
    "X_v -= M\n",
    "X_v /= S\n",
    "\n",
    "# the first dimension is the number of samples\n",
    "print(X_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our first model will be a basic Fully Connected neural net\n",
    "m = keras.Sequential()\n",
    "m.add(kl.Flatten())\n",
    "m.add(kl.Dense(32, activation='relu'))\n",
    "m.add(kl.Dense(32, activation='relu'))\n",
    "# \"softmax\" is a nonlinarity that scales its vector input so that it sums to 1\n",
    "# thus allowing for (mathematically iffy, but practical) interpretation as a vector of probabilities\n",
    "m.add(kl.Dense(Y_tr.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: why do we need the Flatten() call?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n",
      "Fitting model...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3747 - categorical_accuracy: 0.8892 - val_loss: 0.1974 - val_categorical_accuracy: 0.9433\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1772 - categorical_accuracy: 0.9476 - val_loss: 0.1518 - val_categorical_accuracy: 0.9542\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1407 - categorical_accuracy: 0.9581 - val_loss: 0.1474 - val_categorical_accuracy: 0.9522\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1184 - categorical_accuracy: 0.9643 - val_loss: 0.1443 - val_categorical_accuracy: 0.9558\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1038 - categorical_accuracy: 0.9690 - val_loss: 0.1313 - val_categorical_accuracy: 0.9613\n"
     ]
    }
   ],
   "source": [
    "# deep learning models are very computationally intensive, and need to be compiled\n",
    "# and optimized for good performance\n",
    "print(\"Compiling model...\")\n",
    "m.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(\"Fitting model...\")\n",
    "m.fit(X_tr, Y_tr, epochs=5, batch_size=128, validation_data=(X_v, Y_v));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: think about why convolutional neural nets are a better fit for natural images. What properties of physical relaity do they exploit better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise\n",
    "- implement a convolutional neural network\n",
    "- competition! See who can get the highest validation accuracy.\n",
    "- **Exercise**: what's wrong with the line above, from a scientific standpoint. What happens when many differnet models are tried on the same small dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your advanced AI here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
